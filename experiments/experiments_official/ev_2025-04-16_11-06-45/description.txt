This was the first run with the baseline prompt v.1.0. and Llama 4.0.
Only 6 sparql instances of the test set were used. This is because I wanted to first validate that the evaluation pipeline is
working as intended.


Result remarks: 
- I haven't checked the results in detail, but suprisingly once predicted query even got a result back from the endpoint.
- However it has no Service keyword, and thus is not federated. This leads to a recall of 1.0.


Settings: 

Temperature: 1.0
Max tokens: 4000


Prompt: 